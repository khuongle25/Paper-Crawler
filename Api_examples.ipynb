{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f0a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resp.apis.serp_api import Serp\n",
    "from resp.apis.cnnp import connected_papers\n",
    "from resp.apis.semantic_s import Semantic_Scholar\n",
    "from resp.apis.acm_api import ACM\n",
    "from resp.apis.arxiv_api import Arxiv\n",
    "from resp.resp import Resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b27bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paper_names = ['Zero-shot learning with common sense knowledge graphs']\n",
    "keyword     = ['Zero-shot learning']\n",
    "api_key     = 'f2658b409ec094c57f8f5440c6e1bde419164fca9306006b1dffc3aa00abd41b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f677a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs      = Serp(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274256e",
   "metadata": {},
   "source": [
    "### Get all citations of a paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075f9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# collection of all citations\n",
    "result = qs.get_citations(Paper_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6c5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Debiased learning from naturally imbalanced ps...</td>\n",
       "      <td>http://openaccess.thecvf.com/content/CVPR2022/...</td>\n",
       "      <td>This work studies the bias issue of pseudo-lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero-shot and few-shot learning with knowledge...</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "      <td>Machine learning (ML), especially deep neural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What can knowledge bring to machine learning?—...</td>\n",
       "      <td>https://dl.acm.org/doi/abs/10.1145/3510030</td>\n",
       "      <td>Supervised machine learning has several drawba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fusing external knowledge resources for natura...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Abstract Knowledge resources, eg knowledge gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kvl-bert: Knowledge enhanced visual-and-lingui...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Abstract Reasoning is a critical ability towar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Exploring hierarchical graph representation fo...</td>\n",
       "      <td>https://link.springer.com/chapter/10.1007/978-...</td>\n",
       "      <td>The main question we address in this paper is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Commonsense knowledge powered heterogeneous gr...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>In real-world scenarios, considerable human po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Does clip bind concepts? probing compositional...</td>\n",
       "      <td>https://arxiv.org/abs/2212.10537</td>\n",
       "      <td>Large-scale neural network models combining te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Deep Semantic-Visual Alignment for zero-shot r...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Deep neural networks have achieved promising p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A survey on knowledge-enhanced multimodal lear...</td>\n",
       "      <td>https://arxiv.org/abs/2211.12328</td>\n",
       "      <td>Multimodal learning has been a field of increa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Benchmarking knowledge-driven zero-shot learning</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>External knowledge (aka side information) play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Disentangled action recognition with knowledge...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01708</td>\n",
       "      <td>Action in video usually involves the interacti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Knowledge Graphs Meet Multi-Modal Learning: A ...</td>\n",
       "      <td>https://arxiv.org/abs/2402.05391</td>\n",
       "      <td>Knowledge Graphs (KGs) play a pivotal role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Zero-shot node classification with graph contr...</td>\n",
       "      <td>https://openreview.net/forum?id=8wGXnjRLSy</td>\n",
       "      <td>This paper studies zero-shot node classificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Zero-shot Object Classification with Large-sca...</td>\n",
       "      <td>https://openaccess.thecvf.com/content/CVPR2023...</td>\n",
       "      <td>Zero-shot learning is used to predict unseen c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tight lower bounds on worst-case guarantees fo...</td>\n",
       "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
       "      <td>We develop a rigorous mathematical analysis of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Part-Object Progressive Refinement Network for...</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "      <td>Zero-shot learning (ZSL) recognizes unseen ima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Generalised zero-shot learning for entailment-...</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "      <td>Text classification techniques have been subst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CGUN-2A: Deep Graph Convolutional Network via ...</td>\n",
       "      <td>https://www.mdpi.com/1424-8220/22/24/9980</td>\n",
       "      <td>Taxonomy illustrates that natural creatures ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Multi-view graph representation with similarit...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Zero-shot learning (ZSL) aims to predict unsee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Leveraging Knowledge Graphs for Zero-Shot Obje...</td>\n",
       "      <td>https://arxiv.org/abs/2307.12179</td>\n",
       "      <td>We investigate the problem of Object State Cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ProZe: Explainable and Prompt-Guided Zero-Shot...</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "      <td>As technology accelerates the generation and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>面向图神经网络的知识图谱嵌入研究进展.</td>\n",
       "      <td>https://search.ebscohost.com/login.aspx?direct...</td>\n",
       "      <td>随着图神经网络的发展, 基于图神经网络的知识图谱嵌入方法日益受到研究人员的关注. 相比传统的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TAGLETS: A system for automatic semi-supervise...</td>\n",
       "      <td>https://proceedings.mlsys.org/paper_files/pape...</td>\n",
       "      <td>Abstract Machine learning practitioners often ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Top2Label: Explainable zero shot topic labelli...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Automatic topic labelling aims to generate coh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Semantic matching for text classification with...</td>\n",
       "      <td>https://aclanthology.org/2023.emnlp-main.475/</td>\n",
       "      <td>Text classifiers are an indispensable tool for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Towards Neural Network Interpretability Using ...</td>\n",
       "      <td>https://link.springer.com/chapter/10.1007/978-...</td>\n",
       "      <td>Convolutional neural networks (CNNs) classify ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Zero-shot fine-grained entity typing in inform...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>The field of information security suffers from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Shifted Window Based Self-Attention via Swin T...</td>\n",
       "      <td>https://publications.waset.org/10013271/shifte...</td>\n",
       "      <td>Generalised Zero-Shot Learning, often known as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Integrating Noisy Knowledge into Language Repr...</td>\n",
       "      <td>https://ieeexplore.ieee.org/abstract/document/...</td>\n",
       "      <td>Integrating structured knowledge into language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Exploring the Impact of Knowledge Graphs on Ze...</td>\n",
       "      <td>http://users.ics.forth.gr/~argyros/mypapers/20...</td>\n",
       "      <td>In this work, we explore the potential of Know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Knowledge Relevance BERT: Integrating Noisy Kn...</td>\n",
       "      <td>https://knowledge-nlp.github.io/aaai2023/paper...</td>\n",
       "      <td>Integrating structured knowledge into language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>実世界へ挑む機械学習</td>\n",
       "      <td>http://www.nlab.ci.i.u-tokyo.ac.jp/pdf/ieice20...</td>\n",
       "      <td>現在の機械学習の成功を支える中心的なアプローチである教師あり学習では, 対象とするタスクの入...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Debiased learning from naturally imbalanced ps...   \n",
       "1   Zero-shot and few-shot learning with knowledge...   \n",
       "2   What can knowledge bring to machine learning?—...   \n",
       "3   Fusing external knowledge resources for natura...   \n",
       "4   Kvl-bert: Knowledge enhanced visual-and-lingui...   \n",
       "5   Exploring hierarchical graph representation fo...   \n",
       "6   Commonsense knowledge powered heterogeneous gr...   \n",
       "7   Does clip bind concepts? probing compositional...   \n",
       "8   Deep Semantic-Visual Alignment for zero-shot r...   \n",
       "9   A survey on knowledge-enhanced multimodal lear...   \n",
       "10   Benchmarking knowledge-driven zero-shot learning   \n",
       "11  Disentangled action recognition with knowledge...   \n",
       "12  Knowledge Graphs Meet Multi-Modal Learning: A ...   \n",
       "13  Zero-shot node classification with graph contr...   \n",
       "14  Zero-shot Object Classification with Large-sca...   \n",
       "15  Tight lower bounds on worst-case guarantees fo...   \n",
       "16  Part-Object Progressive Refinement Network for...   \n",
       "17  Generalised zero-shot learning for entailment-...   \n",
       "18  CGUN-2A: Deep Graph Convolutional Network via ...   \n",
       "19  Multi-view graph representation with similarit...   \n",
       "20  Leveraging Knowledge Graphs for Zero-Shot Obje...   \n",
       "21  ProZe: Explainable and Prompt-Guided Zero-Shot...   \n",
       "22                                面向图神经网络的知识图谱嵌入研究进展.   \n",
       "23  TAGLETS: A system for automatic semi-supervise...   \n",
       "24  Top2Label: Explainable zero shot topic labelli...   \n",
       "25  Semantic matching for text classification with...   \n",
       "26  Towards Neural Network Interpretability Using ...   \n",
       "27  Zero-shot fine-grained entity typing in inform...   \n",
       "28  Shifted Window Based Self-Attention via Swin T...   \n",
       "29  Integrating Noisy Knowledge into Language Repr...   \n",
       "30  Exploring the Impact of Knowledge Graphs on Ze...   \n",
       "31  Knowledge Relevance BERT: Integrating Noisy Kn...   \n",
       "32                                         実世界へ挑む機械学習   \n",
       "\n",
       "                                                 link  \\\n",
       "0   http://openaccess.thecvf.com/content/CVPR2022/...   \n",
       "1   https://ieeexplore.ieee.org/abstract/document/...   \n",
       "2          https://dl.acm.org/doi/abs/10.1145/3510030   \n",
       "3   https://www.sciencedirect.com/science/article/...   \n",
       "4   https://www.sciencedirect.com/science/article/...   \n",
       "5   https://link.springer.com/chapter/10.1007/978-...   \n",
       "6   https://www.sciencedirect.com/science/article/...   \n",
       "7                    https://arxiv.org/abs/2212.10537   \n",
       "8   https://www.sciencedirect.com/science/article/...   \n",
       "9                    https://arxiv.org/abs/2211.12328   \n",
       "10  https://www.sciencedirect.com/science/article/...   \n",
       "11                   https://arxiv.org/abs/2207.01708   \n",
       "12                   https://arxiv.org/abs/2402.05391   \n",
       "13         https://openreview.net/forum?id=8wGXnjRLSy   \n",
       "14  https://openaccess.thecvf.com/content/CVPR2023...   \n",
       "15  https://proceedings.neurips.cc/paper_files/pap...   \n",
       "16  https://ieeexplore.ieee.org/abstract/document/...   \n",
       "17  https://ieeexplore.ieee.org/abstract/document/...   \n",
       "18          https://www.mdpi.com/1424-8220/22/24/9980   \n",
       "19  https://www.sciencedirect.com/science/article/...   \n",
       "20                   https://arxiv.org/abs/2307.12179   \n",
       "21  https://ieeexplore.ieee.org/abstract/document/...   \n",
       "22  https://search.ebscohost.com/login.aspx?direct...   \n",
       "23  https://proceedings.mlsys.org/paper_files/pape...   \n",
       "24  https://www.sciencedirect.com/science/article/...   \n",
       "25      https://aclanthology.org/2023.emnlp-main.475/   \n",
       "26  https://link.springer.com/chapter/10.1007/978-...   \n",
       "27  https://www.sciencedirect.com/science/article/...   \n",
       "28  https://publications.waset.org/10013271/shifte...   \n",
       "29  https://ieeexplore.ieee.org/abstract/document/...   \n",
       "30  http://users.ics.forth.gr/~argyros/mypapers/20...   \n",
       "31  https://knowledge-nlp.github.io/aaai2023/paper...   \n",
       "32  http://www.nlab.ci.i.u-tokyo.ac.jp/pdf/ieice20...   \n",
       "\n",
       "                                              snippet  \n",
       "0   This work studies the bias issue of pseudo-lab...  \n",
       "1   Machine learning (ML), especially deep neural ...  \n",
       "2   Supervised machine learning has several drawba...  \n",
       "3   Abstract Knowledge resources, eg knowledge gra...  \n",
       "4   Abstract Reasoning is a critical ability towar...  \n",
       "5   The main question we address in this paper is ...  \n",
       "6   In real-world scenarios, considerable human po...  \n",
       "7   Large-scale neural network models combining te...  \n",
       "8   Deep neural networks have achieved promising p...  \n",
       "9   Multimodal learning has been a field of increa...  \n",
       "10  External knowledge (aka side information) play...  \n",
       "11  Action in video usually involves the interacti...  \n",
       "12  Knowledge Graphs (KGs) play a pivotal role in ...  \n",
       "13  This paper studies zero-shot node classificati...  \n",
       "14  Zero-shot learning is used to predict unseen c...  \n",
       "15  We develop a rigorous mathematical analysis of...  \n",
       "16  Zero-shot learning (ZSL) recognizes unseen ima...  \n",
       "17  Text classification techniques have been subst...  \n",
       "18  Taxonomy illustrates that natural creatures ca...  \n",
       "19  Zero-shot learning (ZSL) aims to predict unsee...  \n",
       "20  We investigate the problem of Object State Cla...  \n",
       "21  As technology accelerates the generation and c...  \n",
       "22  随着图神经网络的发展, 基于图神经网络的知识图谱嵌入方法日益受到研究人员的关注. 相比传统的...  \n",
       "23  Abstract Machine learning practitioners often ...  \n",
       "24  Automatic topic labelling aims to generate coh...  \n",
       "25  Text classifiers are an indispensable tool for...  \n",
       "26  Convolutional neural networks (CNNs) classify ...  \n",
       "27  The field of information security suffers from...  \n",
       "28  Generalised Zero-Shot Learning, often known as...  \n",
       "29  Integrating structured knowledge into language...  \n",
       "30  In this work, we explore the potential of Know...  \n",
       "31  Integrating structured knowledge into language...  \n",
       "32  現在の機械学習の成功を支える中心的なアプローチである教師あり学習では, 対象とするタスクの入...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['8b013d0327a9b0a442929e54e06c283a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008b818",
   "metadata": {},
   "source": [
    "### Get all related paper of a single paper from google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931a4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Testing all related papers of a research paper from google scholar\n",
    "rl_result             = qs.get_related_pages(Paper_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095c6a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-shot learning with common sense knowledge...</td>\n",
       "      <td>https://arxiv.org/abs/2006.10713</td>\n",
       "      <td>Zero-shot learning relies on semantic class re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero-shot learning via contrastive learning on...</td>\n",
       "      <td>https://openaccess.thecvf.com/content/ICCV2021...</td>\n",
       "      <td>Abstract Graph Convolutional Networks (GCNs), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Semantic guided knowledge graph for large-scal...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>Zero-shot learning has received growing attent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explainable zero-shot learning via attentive g...</td>\n",
       "      <td>https://content.iospress.com/articles/semantic...</td>\n",
       "      <td>Zero-shot learning (ZSL) which aims to deal wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Attribute propagation network for graph zero-s...</td>\n",
       "      <td>https://aaai.org/ojs/index.php/AAAI/article/vi...</td>\n",
       "      <td>The goal of zero-shot learning (ZSL) is to tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Attribute attention for semantic disambiguatio...</td>\n",
       "      <td>http://openaccess.thecvf.com/content_ICCV_2019...</td>\n",
       "      <td>Zero-shot learning (ZSL) aims to accurately re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Contrastive semantic disentanglement in latent...</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>The target of generalized zero-shot learning (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A joint generative model for zero-shot learning</td>\n",
       "      <td>http://openaccess.thecvf.com/content_eccv_2018...</td>\n",
       "      <td>Zero-shot learning (ZSL) is a challenging task...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I2dformer: Learning image to document attentio...</td>\n",
       "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
       "      <td>Despite the tremendous progress in zero-shot l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Alleviating feature confusion for generative z...</td>\n",
       "      <td>https://dl.acm.org/doi/abs/10.1145/3343031.335...</td>\n",
       "      <td>Lately, generative adversarial networks (GANs)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Zero-shot learning with common sense knowledge...   \n",
       "1   Zero-shot learning via contrastive learning on...   \n",
       "2   Semantic guided knowledge graph for large-scal...   \n",
       "3   Explainable zero-shot learning via attentive g...   \n",
       "4   Attribute propagation network for graph zero-s...   \n",
       "..                                                ...   \n",
       "95  Attribute attention for semantic disambiguatio...   \n",
       "96  Contrastive semantic disentanglement in latent...   \n",
       "97    A joint generative model for zero-shot learning   \n",
       "98  I2dformer: Learning image to document attentio...   \n",
       "99  Alleviating feature confusion for generative z...   \n",
       "\n",
       "                                                 link  \\\n",
       "0                    https://arxiv.org/abs/2006.10713   \n",
       "1   https://openaccess.thecvf.com/content/ICCV2021...   \n",
       "2   https://www.sciencedirect.com/science/article/...   \n",
       "3   https://content.iospress.com/articles/semantic...   \n",
       "4   https://aaai.org/ojs/index.php/AAAI/article/vi...   \n",
       "..                                                ...   \n",
       "95  http://openaccess.thecvf.com/content_ICCV_2019...   \n",
       "96  https://www.sciencedirect.com/science/article/...   \n",
       "97  http://openaccess.thecvf.com/content_eccv_2018...   \n",
       "98  https://proceedings.neurips.cc/paper_files/pap...   \n",
       "99  https://dl.acm.org/doi/abs/10.1145/3343031.335...   \n",
       "\n",
       "                                              snippet  \n",
       "0   Zero-shot learning relies on semantic class re...  \n",
       "1   Abstract Graph Convolutional Networks (GCNs), ...  \n",
       "2   Zero-shot learning has received growing attent...  \n",
       "3   Zero-shot learning (ZSL) which aims to deal wi...  \n",
       "4   The goal of zero-shot learning (ZSL) is to tra...  \n",
       "..                                                ...  \n",
       "95  Zero-shot learning (ZSL) aims to accurately re...  \n",
       "96  The target of generalized zero-shot learning (...  \n",
       "97  Zero-shot learning (ZSL) is a challenging task...  \n",
       "98  Despite the tremendous progress in zero-shot l...  \n",
       "99  Lately, generative adversarial networks (GANs)...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_result['5d0128d1ea70972349587d570158f12c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbcbe01",
   "metadata": {},
   "source": [
    "### get all related papers from connectedpapers.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b6008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url tab opened..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for loading papers result..\n",
      "waiting for graph loading..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:30<00:00, 30.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "Please run again..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# connected papers shows a list based on your search parameter n implies how many paper's graph\n",
    "# you want to extract, usually the first result is extract paper so n = 1 will be good option\n",
    "cp     = connected_papers()\n",
    "papers = cp.download_papers(Paper_names[0], n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60a80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ab3e7",
   "metadata": {},
   "source": [
    "### Get relevant papers from arxiv based on keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bc5b5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.69s/it]\n"
     ]
    }
   ],
   "source": [
    "ap           = Arxiv()\n",
    "arxiv_result = ap.arxiv('Zero-shot learning', max_pages = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61fae8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Contribution of Lyrics and Acoustics to Co...</td>\n",
       "      <td>https://arxiv.org/abs/2207.05680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CodeRL: Mastering Code Generation through Pret...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General Policy Evaluation and Improvement by L...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter-Efficient Prompt Tuning Makes Genera...</td>\n",
       "      <td>https://arxiv.org/abs/2207.07087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Personalized Zero-Shot ECG Arrhythmia Monito...</td>\n",
       "      <td>https://arxiv.org/abs/2207.07089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Glow-WaveGAN 2: High-quality Zero-shot Text-to...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adaptive Fine-Grained Sketch-Based Image Retri...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GSMFlow: Generation Shifts Mitigating Flow for...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Memory Efficient Patch-based Training for INR-...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>REZCR: A Zero-shot Character Recognition Metho...</td>\n",
       "      <td>https://arxiv.org/abs/2207.05842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PROTOtypical Logic Tensor Networks (PROTO-LTN)...</td>\n",
       "      <td>https://arxiv.org/abs/2207.00433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Learning the Stress-Strain Fields in Digital C...</td>\n",
       "      <td>https://arxiv.org/abs/2207.03239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DUET: Cross-modal Semantic Grounding for Contr...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Randomized-to-Canonical Model Predictive Contr...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Boosting Zero-shot Learning via Contrastive Op...</td>\n",
       "      <td>https://arxiv.org/abs/2207.03824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Few 'Zero Level Set'-Shot Learning of Shape Si...</td>\n",
       "      <td>https://arxiv.org/abs/2207.04161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Discourse-Aware Graph Networks for Textual Log...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A Multi-Task BERT Model for Schema-Guided Dial...</td>\n",
       "      <td>https://arxiv.org/abs/2207.00828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SATTS: Speaker Attractor Text to Speech, Learn...</td>\n",
       "      <td>https://arxiv.org/abs/2207.06011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Strong Heuristics for Named Entity Linking</td>\n",
       "      <td>https://arxiv.org/abs/2207.02824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Open-Vocabulary Multi-Label Classification via...</td>\n",
       "      <td>https://arxiv.org/abs/2207.01887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Zero-shot Cross-Linguistic Learning of Event S...</td>\n",
       "      <td>https://arxiv.org/abs/2207.02356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Can Language Understand Depth?</td>\n",
       "      <td>https://arxiv.org/abs/2207.01077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Few-shot training LLMs for project-specific co...</td>\n",
       "      <td>https://arxiv.org/abs/2207.04237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Models Out of Line: A Fourier Lens on Distribu...</td>\n",
       "      <td>https://arxiv.org/abs/2207.04075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cross-View Language Modeling: Towards Unified ...</td>\n",
       "      <td>https://arxiv.org/abs/2206.00621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SHRED: 3D Shape Region Decomposition with Lear...</td>\n",
       "      <td>https://arxiv.org/abs/2206.03480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Model Generation with Provable Coverability fo...</td>\n",
       "      <td>https://arxiv.org/abs/2206.00316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SNAKE: Shape-aware Neural 3D Keypoint Field</td>\n",
       "      <td>https://arxiv.org/abs/2206.01724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Language Models are General-Purpose Interfaces</td>\n",
       "      <td>https://arxiv.org/abs/2206.06336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GLIPv2: Unifying Localization and Vision-Langu...</td>\n",
       "      <td>https://arxiv.org/abs/2206.05836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RACA: Relation-Aware Credit Assignment for Ad-...</td>\n",
       "      <td>https://arxiv.org/abs/2206.01207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>How Far I'll Go: Offline Goal-Conditioned Rein...</td>\n",
       "      <td>https://arxiv.org/abs/2206.03023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Prompt Injection: Parameterization of Fixed In...</td>\n",
       "      <td>https://arxiv.org/abs/2206.11349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Prefix Language Models are Unified Modal Learners</td>\n",
       "      <td>https://arxiv.org/abs/2206.07699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Modularized Transfer Learning with Multiple Kn...</td>\n",
       "      <td>https://arxiv.org/abs/2206.03715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Trial2Vec: Zero-Shot Clinical Trial Document S...</td>\n",
       "      <td>https://arxiv.org/abs/2206.14719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Uni-Perceiver-MoE: Learning Sparse Generalist ...</td>\n",
       "      <td>https://arxiv.org/abs/2206.04674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GenSDF: Two-Stage Learning of Generalizable Si...</td>\n",
       "      <td>https://arxiv.org/abs/2206.02780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Medical Coding with Biomedical Transformer Ens...</td>\n",
       "      <td>https://arxiv.org/abs/2206.02662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Transductive CLIP with Class-Conditional Contr...</td>\n",
       "      <td>https://arxiv.org/abs/2206.06177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Disentangled Ontology Embedding for Zero-shot ...</td>\n",
       "      <td>https://arxiv.org/abs/2206.03739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>BigVGAN: A Universal Neural Vocoder with Large...</td>\n",
       "      <td>https://arxiv.org/abs/2206.04658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Zero-shot object goal visual navigation</td>\n",
       "      <td>https://arxiv.org/abs/2206.07423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Self-Generated In-Context Learning: Leveraging...</td>\n",
       "      <td>https://arxiv.org/abs/2206.08082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Transfer Language Selection for Zero-Shot Cros...</td>\n",
       "      <td>https://arxiv.org/abs/2206.00962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Learning Using Privileged Information for Zero...</td>\n",
       "      <td>https://arxiv.org/abs/2206.08632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Template-based Approach to Zero-shot Intent Re...</td>\n",
       "      <td>https://arxiv.org/abs/2206.10914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ZSON: Zero-Shot Object-Goal Navigation using M...</td>\n",
       "      <td>https://arxiv.org/abs/2206.12403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Zero-Shot AutoML with Pretrained Models</td>\n",
       "      <td>https://arxiv.org/abs/2206.08476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   The Contribution of Lyrics and Acoustics to Co...   \n",
       "1   CodeRL: Mastering Code Generation through Pret...   \n",
       "2   General Policy Evaluation and Improvement by L...   \n",
       "3   Parameter-Efficient Prompt Tuning Makes Genera...   \n",
       "4   A Personalized Zero-Shot ECG Arrhythmia Monito...   \n",
       "5   Glow-WaveGAN 2: High-quality Zero-shot Text-to...   \n",
       "6   Adaptive Fine-Grained Sketch-Based Image Retri...   \n",
       "7   GSMFlow: Generation Shifts Mitigating Flow for...   \n",
       "8   Memory Efficient Patch-based Training for INR-...   \n",
       "9   REZCR: A Zero-shot Character Recognition Metho...   \n",
       "10  PROTOtypical Logic Tensor Networks (PROTO-LTN)...   \n",
       "11  Learning the Stress-Strain Fields in Digital C...   \n",
       "12  DUET: Cross-modal Semantic Grounding for Contr...   \n",
       "13  Randomized-to-Canonical Model Predictive Contr...   \n",
       "14  Boosting Zero-shot Learning via Contrastive Op...   \n",
       "15  Few 'Zero Level Set'-Shot Learning of Shape Si...   \n",
       "16  Discourse-Aware Graph Networks for Textual Log...   \n",
       "17  A Multi-Task BERT Model for Schema-Guided Dial...   \n",
       "18  SATTS: Speaker Attractor Text to Speech, Learn...   \n",
       "19         Strong Heuristics for Named Entity Linking   \n",
       "20  Open-Vocabulary Multi-Label Classification via...   \n",
       "21  Zero-shot Cross-Linguistic Learning of Event S...   \n",
       "22                     Can Language Understand Depth?   \n",
       "23  Few-shot training LLMs for project-specific co...   \n",
       "24  Models Out of Line: A Fourier Lens on Distribu...   \n",
       "25  Cross-View Language Modeling: Towards Unified ...   \n",
       "26  SHRED: 3D Shape Region Decomposition with Lear...   \n",
       "27  Model Generation with Provable Coverability fo...   \n",
       "28        SNAKE: Shape-aware Neural 3D Keypoint Field   \n",
       "29     Language Models are General-Purpose Interfaces   \n",
       "30  GLIPv2: Unifying Localization and Vision-Langu...   \n",
       "31  RACA: Relation-Aware Credit Assignment for Ad-...   \n",
       "32  How Far I'll Go: Offline Goal-Conditioned Rein...   \n",
       "33  Prompt Injection: Parameterization of Fixed In...   \n",
       "34  Prefix Language Models are Unified Modal Learners   \n",
       "35  Modularized Transfer Learning with Multiple Kn...   \n",
       "36  Trial2Vec: Zero-Shot Clinical Trial Document S...   \n",
       "37  Uni-Perceiver-MoE: Learning Sparse Generalist ...   \n",
       "38  GenSDF: Two-Stage Learning of Generalizable Si...   \n",
       "39  Medical Coding with Biomedical Transformer Ens...   \n",
       "40  Transductive CLIP with Class-Conditional Contr...   \n",
       "41  Disentangled Ontology Embedding for Zero-shot ...   \n",
       "42  BigVGAN: A Universal Neural Vocoder with Large...   \n",
       "43            Zero-shot object goal visual navigation   \n",
       "44  Self-Generated In-Context Learning: Leveraging...   \n",
       "45  Transfer Language Selection for Zero-Shot Cros...   \n",
       "46  Learning Using Privileged Information for Zero...   \n",
       "47  Template-based Approach to Zero-shot Intent Re...   \n",
       "48  ZSON: Zero-Shot Object-Goal Navigation using M...   \n",
       "49            Zero-Shot AutoML with Pretrained Models   \n",
       "\n",
       "                                link  \n",
       "0   https://arxiv.org/abs/2207.05680  \n",
       "1   https://arxiv.org/abs/2207.01780  \n",
       "2   https://arxiv.org/abs/2207.01566  \n",
       "3   https://arxiv.org/abs/2207.07087  \n",
       "4   https://arxiv.org/abs/2207.07089  \n",
       "5   https://arxiv.org/abs/2207.01832  \n",
       "6   https://arxiv.org/abs/2207.01723  \n",
       "7   https://arxiv.org/abs/2207.01798  \n",
       "8   https://arxiv.org/abs/2207.01395  \n",
       "9   https://arxiv.org/abs/2207.05842  \n",
       "10  https://arxiv.org/abs/2207.00433  \n",
       "11  https://arxiv.org/abs/2207.03239  \n",
       "12  https://arxiv.org/abs/2207.01328  \n",
       "13  https://arxiv.org/abs/2207.01840  \n",
       "14  https://arxiv.org/abs/2207.03824  \n",
       "15  https://arxiv.org/abs/2207.04161  \n",
       "16  https://arxiv.org/abs/2207.01450  \n",
       "17  https://arxiv.org/abs/2207.00828  \n",
       "18  https://arxiv.org/abs/2207.06011  \n",
       "19  https://arxiv.org/abs/2207.02824  \n",
       "20  https://arxiv.org/abs/2207.01887  \n",
       "21  https://arxiv.org/abs/2207.02356  \n",
       "22  https://arxiv.org/abs/2207.01077  \n",
       "23  https://arxiv.org/abs/2207.04237  \n",
       "24  https://arxiv.org/abs/2207.04075  \n",
       "25  https://arxiv.org/abs/2206.00621  \n",
       "26  https://arxiv.org/abs/2206.03480  \n",
       "27  https://arxiv.org/abs/2206.00316  \n",
       "28  https://arxiv.org/abs/2206.01724  \n",
       "29  https://arxiv.org/abs/2206.06336  \n",
       "30  https://arxiv.org/abs/2206.05836  \n",
       "31  https://arxiv.org/abs/2206.01207  \n",
       "32  https://arxiv.org/abs/2206.03023  \n",
       "33  https://arxiv.org/abs/2206.11349  \n",
       "34  https://arxiv.org/abs/2206.07699  \n",
       "35  https://arxiv.org/abs/2206.03715  \n",
       "36  https://arxiv.org/abs/2206.14719  \n",
       "37  https://arxiv.org/abs/2206.04674  \n",
       "38  https://arxiv.org/abs/2206.02780  \n",
       "39  https://arxiv.org/abs/2206.02662  \n",
       "40  https://arxiv.org/abs/2206.06177  \n",
       "41  https://arxiv.org/abs/2206.03739  \n",
       "42  https://arxiv.org/abs/2206.04658  \n",
       "43  https://arxiv.org/abs/2206.07423  \n",
       "44  https://arxiv.org/abs/2206.08082  \n",
       "45  https://arxiv.org/abs/2206.00962  \n",
       "46  https://arxiv.org/abs/2206.08632  \n",
       "47  https://arxiv.org/abs/2206.10914  \n",
       "48  https://arxiv.org/abs/2206.12403  \n",
       "49  https://arxiv.org/abs/2206.08476  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c20605",
   "metadata": {},
   "source": [
    "### Get relevant papers from ACM digital library based on keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4d74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.91s/it]\n"
     ]
    }
   ],
   "source": [
    "ac           = ACM()\n",
    "acm_result   = ac.acm('Zero-shot learning', max_pages = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb29044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Active Zero-Shot Learning</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/2983323.298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OntoZSL: Ontology-enhanced Zero-shot Learning</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3442381.345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZSGL: zero shot gestural learning</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3136755.313...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Survey of Zero-Shot Learning: Settings, Meth...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3293318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zero-Shot Learning for Gesture Recognition</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3382507.342...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zero-shot program representation learning</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3524610.352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Disentangled Ontology Embedding for Zero-shot ...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3534678.353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Salient Latent Features For Zero-shot Learning</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3402597.340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Residual Graph Convolutional Networks for Zero...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3338533.336...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Imagination Based Sample Construction for Zero...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3209978.321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Improved Feature Generating Networks for Zero-...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3531232.353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alleviating Feature Confusion for Generative Z...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3343031.335...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Transductive Visual-Semantic Embedding for Zer...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3078971.307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Distributed representation of tags for Active ...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3430984.343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Prompt-based Zero-shot Video Moment Retrieval</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3503161.354...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Zero-shot Relation Classification from Side In...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3459637.348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Zero Shot Transfer Learning for Robot Soccer</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.5555/3237383.323...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Multimodal Zero-Shot Hateful Meme Detection</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3501247.353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Trajectory Diversity for Zero-Shot Coordination</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.5555/3463952.346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Neural Zero-Shot Fine-Grained Entity Typing</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3366424.338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Region Semantically Aligned Network for Zero-S...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3459637.348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Dual Part Discovery Network for Zero-Shot Lear...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3503161.354...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Generalized Zero-Shot Extreme Multi-label Lear...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3447548.346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mitigating Generation Shifts for Generalized Z...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3474085.347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Zero-Shot Hashing via Transferring Supervised ...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/2964284.296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Zero-Shot Audio Classification Via Semantic Em...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1109/TASLP.2021....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>VAE-GAN Based Zero-shot Outlier Detection</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3440084.344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Visual Language Based Succinct Zero-Shot Objec...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3474085.347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Pseudo Transfer with Marginalized Corrupted At...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3240508.324...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Zero-Shot Stance Detection via Contrastive Lea...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3485447.351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Graph-based variational auto-encoder for gener...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3444685.344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Generative Adversarial Zero-Shot Learning for ...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3511808.355...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Visual context embeddings for zero-shot recogn...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3477314.350...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Zero-Shot Image Classification via Consistent ...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3398329.339...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Identifying Entity Properties from Text with Z...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3331184.333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Semantic Enhanced Cross-modal GAN for Zero-sho...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3469877.349...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Zero-Shot Cross-Lingual Neural Headline Genera...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1109/TASLP.2018....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Zero-shot Image Categorization by Image Correl...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/2671188.274...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Learning Modality-Invariant Latent Representat...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3394171.341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Dissimilarity Representation Learning for Gene...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3240508.324...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Generalized Zero-Shot Video Classification via...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3394171.341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Zero-shot reductive paraphrasing for digitally...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3503162.350...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Context-aware Feature Generation For Zero-shot...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3394171.341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Domain-Specific Embedding Network for Zero-Sho...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3343031.335...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Zero-shot Image Tagging by Hierarchical Semant...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/2766462.276...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Reinforced Zero-Shot Cross-Lingual Neural Head...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1109/TASLP.2020....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>KG-ZESHEL: Knowledge Graph-Enhanced Zero-Shot ...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3460210.349...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Zero-shot Node Classification with Decomposed ...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3447548.346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Zero-Shot Learning for IMU-Based Activity Reco...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3494995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>A Contrastive Learning Approach for Compositio...</td>\n",
       "      <td>https://dl.acm.org/doi/pdf/10.1145/3462244.347...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                           Active Zero-Shot Learning   \n",
       "1       OntoZSL: Ontology-enhanced Zero-shot Learning   \n",
       "2                   ZSGL: zero shot gestural learning   \n",
       "3   A Survey of Zero-Shot Learning: Settings, Meth...   \n",
       "4          Zero-Shot Learning for Gesture Recognition   \n",
       "5           Zero-shot program representation learning   \n",
       "6   Disentangled Ontology Embedding for Zero-shot ...   \n",
       "7      Salient Latent Features For Zero-shot Learning   \n",
       "8   Residual Graph Convolutional Networks for Zero...   \n",
       "9   Imagination Based Sample Construction for Zero...   \n",
       "10  Improved Feature Generating Networks for Zero-...   \n",
       "11  Alleviating Feature Confusion for Generative Z...   \n",
       "12  Transductive Visual-Semantic Embedding for Zer...   \n",
       "13  Distributed representation of tags for Active ...   \n",
       "14      Prompt-based Zero-shot Video Moment Retrieval   \n",
       "15  Zero-shot Relation Classification from Side In...   \n",
       "16       Zero Shot Transfer Learning for Robot Soccer   \n",
       "17        Multimodal Zero-Shot Hateful Meme Detection   \n",
       "18    Trajectory Diversity for Zero-Shot Coordination   \n",
       "19        Neural Zero-Shot Fine-Grained Entity Typing   \n",
       "20  Region Semantically Aligned Network for Zero-S...   \n",
       "21  Dual Part Discovery Network for Zero-Shot Lear...   \n",
       "22  Generalized Zero-Shot Extreme Multi-label Lear...   \n",
       "23  Mitigating Generation Shifts for Generalized Z...   \n",
       "24  Zero-Shot Hashing via Transferring Supervised ...   \n",
       "25  Zero-Shot Audio Classification Via Semantic Em...   \n",
       "26          VAE-GAN Based Zero-shot Outlier Detection   \n",
       "27  Visual Language Based Succinct Zero-Shot Objec...   \n",
       "28  Pseudo Transfer with Marginalized Corrupted At...   \n",
       "29  Zero-Shot Stance Detection via Contrastive Lea...   \n",
       "30  Graph-based variational auto-encoder for gener...   \n",
       "31  Generative Adversarial Zero-Shot Learning for ...   \n",
       "32  Visual context embeddings for zero-shot recogn...   \n",
       "33  Zero-Shot Image Classification via Consistent ...   \n",
       "34  Identifying Entity Properties from Text with Z...   \n",
       "35  Semantic Enhanced Cross-modal GAN for Zero-sho...   \n",
       "36  Zero-Shot Cross-Lingual Neural Headline Genera...   \n",
       "37  Zero-shot Image Categorization by Image Correl...   \n",
       "38  Learning Modality-Invariant Latent Representat...   \n",
       "39  Dissimilarity Representation Learning for Gene...   \n",
       "40  Generalized Zero-Shot Video Classification via...   \n",
       "41  Zero-shot reductive paraphrasing for digitally...   \n",
       "42  Context-aware Feature Generation For Zero-shot...   \n",
       "43  Domain-Specific Embedding Network for Zero-Sho...   \n",
       "44  Zero-shot Image Tagging by Hierarchical Semant...   \n",
       "45  Reinforced Zero-Shot Cross-Lingual Neural Head...   \n",
       "46  KG-ZESHEL: Knowledge Graph-Enhanced Zero-Shot ...   \n",
       "47  Zero-shot Node Classification with Decomposed ...   \n",
       "48  Zero-Shot Learning for IMU-Based Activity Reco...   \n",
       "49  A Contrastive Learning Approach for Compositio...   \n",
       "\n",
       "                                                 link  \n",
       "0   https://dl.acm.org/doi/pdf/10.1145/2983323.298...  \n",
       "1   https://dl.acm.org/doi/pdf/10.1145/3442381.345...  \n",
       "2   https://dl.acm.org/doi/pdf/10.1145/3136755.313...  \n",
       "3          https://dl.acm.org/doi/pdf/10.1145/3293318  \n",
       "4   https://dl.acm.org/doi/pdf/10.1145/3382507.342...  \n",
       "5   https://dl.acm.org/doi/pdf/10.1145/3524610.352...  \n",
       "6   https://dl.acm.org/doi/pdf/10.1145/3534678.353...  \n",
       "7   https://dl.acm.org/doi/pdf/10.1145/3402597.340...  \n",
       "8   https://dl.acm.org/doi/pdf/10.1145/3338533.336...  \n",
       "9   https://dl.acm.org/doi/pdf/10.1145/3209978.321...  \n",
       "10  https://dl.acm.org/doi/pdf/10.1145/3531232.353...  \n",
       "11  https://dl.acm.org/doi/pdf/10.1145/3343031.335...  \n",
       "12  https://dl.acm.org/doi/pdf/10.1145/3078971.307...  \n",
       "13  https://dl.acm.org/doi/pdf/10.1145/3430984.343...  \n",
       "14  https://dl.acm.org/doi/pdf/10.1145/3503161.354...  \n",
       "15  https://dl.acm.org/doi/pdf/10.1145/3459637.348...  \n",
       "16  https://dl.acm.org/doi/pdf/10.5555/3237383.323...  \n",
       "17  https://dl.acm.org/doi/pdf/10.1145/3501247.353...  \n",
       "18  https://dl.acm.org/doi/pdf/10.5555/3463952.346...  \n",
       "19  https://dl.acm.org/doi/pdf/10.1145/3366424.338...  \n",
       "20  https://dl.acm.org/doi/pdf/10.1145/3459637.348...  \n",
       "21  https://dl.acm.org/doi/pdf/10.1145/3503161.354...  \n",
       "22  https://dl.acm.org/doi/pdf/10.1145/3447548.346...  \n",
       "23  https://dl.acm.org/doi/pdf/10.1145/3474085.347...  \n",
       "24  https://dl.acm.org/doi/pdf/10.1145/2964284.296...  \n",
       "25  https://dl.acm.org/doi/pdf/10.1109/TASLP.2021....  \n",
       "26  https://dl.acm.org/doi/pdf/10.1145/3440084.344...  \n",
       "27  https://dl.acm.org/doi/pdf/10.1145/3474085.347...  \n",
       "28  https://dl.acm.org/doi/pdf/10.1145/3240508.324...  \n",
       "29  https://dl.acm.org/doi/pdf/10.1145/3485447.351...  \n",
       "30  https://dl.acm.org/doi/pdf/10.1145/3444685.344...  \n",
       "31  https://dl.acm.org/doi/pdf/10.1145/3511808.355...  \n",
       "32  https://dl.acm.org/doi/pdf/10.1145/3477314.350...  \n",
       "33  https://dl.acm.org/doi/pdf/10.1145/3398329.339...  \n",
       "34  https://dl.acm.org/doi/pdf/10.1145/3331184.333...  \n",
       "35  https://dl.acm.org/doi/pdf/10.1145/3469877.349...  \n",
       "36  https://dl.acm.org/doi/pdf/10.1109/TASLP.2018....  \n",
       "37  https://dl.acm.org/doi/pdf/10.1145/2671188.274...  \n",
       "38  https://dl.acm.org/doi/pdf/10.1145/3394171.341...  \n",
       "39  https://dl.acm.org/doi/pdf/10.1145/3240508.324...  \n",
       "40  https://dl.acm.org/doi/pdf/10.1145/3394171.341...  \n",
       "41  https://dl.acm.org/doi/pdf/10.1145/3503162.350...  \n",
       "42  https://dl.acm.org/doi/pdf/10.1145/3394171.341...  \n",
       "43  https://dl.acm.org/doi/pdf/10.1145/3343031.335...  \n",
       "44  https://dl.acm.org/doi/pdf/10.1145/2766462.276...  \n",
       "45  https://dl.acm.org/doi/pdf/10.1109/TASLP.2020....  \n",
       "46  https://dl.acm.org/doi/pdf/10.1145/3460210.349...  \n",
       "47  https://dl.acm.org/doi/pdf/10.1145/3447548.346...  \n",
       "48         https://dl.acm.org/doi/pdf/10.1145/3494995  \n",
       "49  https://dl.acm.org/doi/pdf/10.1145/3462244.347...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ca08c",
   "metadata": {},
   "source": [
    "### Get relevant papers from Semantic_Scholar based on keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3608a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.86s/it]\n"
     ]
    }
   ],
   "source": [
    "sc           = Semantic_Scholar()\n",
    "sc_result    = sc.ss('Zero-shot learning', max_pages = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e714eb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot Learning—A Comprehensive Evaluation ...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature Generating Networks for Zero-Shot Lear...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An embarrassingly simple approach to zero-shot...</td>\n",
       "      <td>https://doi.org/10.1007/978-3-319-50077-5_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zero-Shot Learning — The Good, the Bad and the...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semantic Autoencoder for Zero-Shot Learning</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prototypical Networks for Few-shot Learning</td>\n",
       "      <td>https://arxiv.org/pdf/1703.05175.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Synthesized Classifiers for Zero-Shot Learning</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Learning a Deep Embedding Model for Zero-Shot ...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Learning Graph Embeddings for Compositional Ze...</td>\n",
       "      <td>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZeroGen: Efficient Zero-shot Learning via Data...</td>\n",
       "      <td>https://www.aclanthology.org/2022.emnlp-main.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Zero-Shot Learning—A Comprehensive Evaluation ...   \n",
       "1  Feature Generating Networks for Zero-Shot Lear...   \n",
       "2  An embarrassingly simple approach to zero-shot...   \n",
       "3  Zero-Shot Learning — The Good, the Bad and the...   \n",
       "4        Semantic Autoencoder for Zero-Shot Learning   \n",
       "5        Prototypical Networks for Few-shot Learning   \n",
       "6     Synthesized Classifiers for Zero-Shot Learning   \n",
       "7  Learning a Deep Embedding Model for Zero-Shot ...   \n",
       "8  Learning Graph Embeddings for Compositional Ze...   \n",
       "9  ZeroGen: Efficient Zero-shot Learning via Data...   \n",
       "\n",
       "                                                link  \n",
       "0  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...  \n",
       "1  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...  \n",
       "2        https://doi.org/10.1007/978-3-319-50077-5_2  \n",
       "3  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...  \n",
       "4  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...  \n",
       "5               https://arxiv.org/pdf/1703.05175.pdf  \n",
       "6  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...  \n",
       "7  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...  \n",
       "8  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=...  \n",
       "9  https://www.aclanthology.org/2022.emnlp-main.8...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5880338",
   "metadata": {},
   "source": [
    "### Get relevant papers from ACL based on keyword ( based on serp_api )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d45af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_engine = Resp(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f6b6cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page : 1\n",
      "https://serpapi.com/search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page : 2\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "acl_result = paper_engine.acl('Zero-shot learning', max_pages = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78d68907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot Learning of Language Models for Desc...</td>\n",
       "      <td>https://aclanthology.org/Y14-1012.pdf</td>\n",
       "      <td>We propose a novel framework for zero-shot lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero-shot Learning of Classifiers from Natural...</td>\n",
       "      <td>https://aclanthology.org/P18-1029</td>\n",
       "      <td>Humans can efficiently learn new concepts usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zshot: An Open-source Framework for Zero-Shot ...</td>\n",
       "      <td>https://aclanthology.org/2023.acl-demo.34</td>\n",
       "      <td>Abstract. The Zero-Shot Learning (ZSL) task pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Learn to Adapt for Generalized Zero-Shot Text ...</td>\n",
       "      <td>https://aclanthology.org/2022.acl-long.39.pdf</td>\n",
       "      <td>Abstract. Generalized zero-shot text classific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Respectful or Toxic? Using Zero-Shot Learning ...</td>\n",
       "      <td>https://aclanthology.org/2023.woah-1.6</td>\n",
       "      <td>This paper explores zero-shot learning with pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Textual Description Needs to be Refined for Ze...</td>\n",
       "      <td>https://aclanthology.org/2022.findings-emnlp.455</td>\n",
       "      <td>Abstract. Zero-Shot Learning (ZSL) has shown g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ZeroGen: Efficient Zero-shot Learning via Data...</td>\n",
       "      <td>https://aclanthology.org/2022.emnlp-main.801</td>\n",
       "      <td>In this paper, we study a flexible and efficie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MULTIINSTRUCT: Improving Multi-Modal Zero-Shot...</td>\n",
       "      <td>https://aclanthology.org/2023.acl-long.641.pdf</td>\n",
       "      <td>Experimental re- sults demonstrate strong zero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>An Incremental Zero-shot Learning Approach for...</td>\n",
       "      <td>https://aclanthology.org/2022.bea-1.8</td>\n",
       "      <td>Abstract. Peer assessment is an effective and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zero-shot Learning for Grapheme to Phoneme Con...</td>\n",
       "      <td>https://aclanthology.org/2022.findings-acl.166</td>\n",
       "      <td>This work attempts to apply zero-shot learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZeroGen: Efficient Zero-shot Learning via Data...</td>\n",
       "      <td>https://aclanthology.org/2022.emnlp-main.801</td>\n",
       "      <td>In this paper, we study a flexible and efficie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Zero-shot Text Classification via Reinforced S...</td>\n",
       "      <td>https://aclanthology.org/2020.acl-main.272.pdf</td>\n",
       "      <td>Zero-shot learning has been a tough problem si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Few-shot and Zero-shot Approaches to Legal Tex...</td>\n",
       "      <td>https://aclanthology.org/2021.nllp-1.10.pdf</td>\n",
       "      <td>A criticism of such NLP-based approaches to pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Extreme Zero-Shot Learning for Extreme Text Cl...</td>\n",
       "      <td>https://aclanthology.org/2022.naacl-main.399.pdf</td>\n",
       "      <td>The eXtreme Multi-label text Classification. (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Textual Description Needs to be Refined for Ze...</td>\n",
       "      <td>https://aclanthology.org/2022.findings-emnlp.455</td>\n",
       "      <td>Zero-Shot Learning (ZSL) has shown great promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Zero-Shot Transfer Learning for Event Extraction</td>\n",
       "      <td>https://aclanthology.org/P18-1201.pdf</td>\n",
       "      <td>Most previous supervised event extraction meth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Zero-shot Learning for Grapheme to Phoneme Con...</td>\n",
       "      <td>https://aclanthology.org/2022.findings-acl.166</td>\n",
       "      <td>Most existing work focuses heavily on language...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The COT COLLECTION: Improving Zero-shot and Fe...</td>\n",
       "      <td>https://aclanthology.org/2023.emnlp-main.782.pdf</td>\n",
       "      <td>In the zero-shot learning setting, CoT-T5 (3B ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>An Incremental Zero-shot Learning Approach for...</td>\n",
       "      <td>https://aclanthology.org/2022.bea-1.8</td>\n",
       "      <td>In other words, collecting adequate peer feedb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Don't Prompt, Search! Mining-based Zero-Shot L...</td>\n",
       "      <td>https://aclanthology.org/2022.emnlp-main.509</td>\n",
       "      <td>In this paper, we propose an alternative minin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Zero-Shot Learning of Language Models for Desc...   \n",
       "1   Zero-shot Learning of Classifiers from Natural...   \n",
       "2   Zshot: An Open-source Framework for Zero-Shot ...   \n",
       "3   Learn to Adapt for Generalized Zero-Shot Text ...   \n",
       "4   Respectful or Toxic? Using Zero-Shot Learning ...   \n",
       "5   Textual Description Needs to be Refined for Ze...   \n",
       "6   ZeroGen: Efficient Zero-shot Learning via Data...   \n",
       "7   MULTIINSTRUCT: Improving Multi-Modal Zero-Shot...   \n",
       "8   An Incremental Zero-shot Learning Approach for...   \n",
       "9   Zero-shot Learning for Grapheme to Phoneme Con...   \n",
       "10  ZeroGen: Efficient Zero-shot Learning via Data...   \n",
       "11  Zero-shot Text Classification via Reinforced S...   \n",
       "12  Few-shot and Zero-shot Approaches to Legal Tex...   \n",
       "13  Extreme Zero-Shot Learning for Extreme Text Cl...   \n",
       "14  Textual Description Needs to be Refined for Ze...   \n",
       "15   Zero-Shot Transfer Learning for Event Extraction   \n",
       "16  Zero-shot Learning for Grapheme to Phoneme Con...   \n",
       "17  The COT COLLECTION: Improving Zero-shot and Fe...   \n",
       "18  An Incremental Zero-shot Learning Approach for...   \n",
       "19  Don't Prompt, Search! Mining-based Zero-Shot L...   \n",
       "\n",
       "                                                link  \\\n",
       "0              https://aclanthology.org/Y14-1012.pdf   \n",
       "1                  https://aclanthology.org/P18-1029   \n",
       "2          https://aclanthology.org/2023.acl-demo.34   \n",
       "3      https://aclanthology.org/2022.acl-long.39.pdf   \n",
       "4             https://aclanthology.org/2023.woah-1.6   \n",
       "5   https://aclanthology.org/2022.findings-emnlp.455   \n",
       "6       https://aclanthology.org/2022.emnlp-main.801   \n",
       "7     https://aclanthology.org/2023.acl-long.641.pdf   \n",
       "8              https://aclanthology.org/2022.bea-1.8   \n",
       "9     https://aclanthology.org/2022.findings-acl.166   \n",
       "10      https://aclanthology.org/2022.emnlp-main.801   \n",
       "11    https://aclanthology.org/2020.acl-main.272.pdf   \n",
       "12       https://aclanthology.org/2021.nllp-1.10.pdf   \n",
       "13  https://aclanthology.org/2022.naacl-main.399.pdf   \n",
       "14  https://aclanthology.org/2022.findings-emnlp.455   \n",
       "15             https://aclanthology.org/P18-1201.pdf   \n",
       "16    https://aclanthology.org/2022.findings-acl.166   \n",
       "17  https://aclanthology.org/2023.emnlp-main.782.pdf   \n",
       "18             https://aclanthology.org/2022.bea-1.8   \n",
       "19      https://aclanthology.org/2022.emnlp-main.509   \n",
       "\n",
       "                                              snippet  \n",
       "0   We propose a novel framework for zero-shot lea...  \n",
       "1   Humans can efficiently learn new concepts usin...  \n",
       "2   Abstract. The Zero-Shot Learning (ZSL) task pe...  \n",
       "3   Abstract. Generalized zero-shot text classific...  \n",
       "4   This paper explores zero-shot learning with pr...  \n",
       "5   Abstract. Zero-Shot Learning (ZSL) has shown g...  \n",
       "6   In this paper, we study a flexible and efficie...  \n",
       "7   Experimental re- sults demonstrate strong zero...  \n",
       "8   Abstract. Peer assessment is an effective and ...  \n",
       "9   This work attempts to apply zero-shot learning...  \n",
       "10  In this paper, we study a flexible and efficie...  \n",
       "11  Zero-shot learning has been a tough problem si...  \n",
       "12  A criticism of such NLP-based approaches to pr...  \n",
       "13  The eXtreme Multi-label text Classification. (...  \n",
       "14  Zero-Shot Learning (ZSL) has shown great promi...  \n",
       "15  Most previous supervised event extraction meth...  \n",
       "16  Most existing work focuses heavily on language...  \n",
       "17  In the zero-shot learning setting, CoT-T5 (3B ...  \n",
       "18  In other words, collecting adequate peer feedb...  \n",
       "19  In this paper, we propose an alternative minin...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bba62c",
   "metadata": {},
   "source": [
    "### Get relevant papers from PMLR based on keyword ( based on serp_api )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c43501eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page : 1\n",
      "https://serpapi.com/search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page : 2\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "### Get relevant papers from PMLR based on keyword ( based on serp_api )\n",
    "pmlr_result = paper_engine.pmlr('Zero-shot learning', max_pages = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d479b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An embarrassingly simple approach to zero-shot...</td>\n",
       "      <td>https://proceedings.mlr.press/v37/romera-pared...</td>\n",
       "      <td>Zero-shot learning consists in learning how to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDM-Net: A Simple and Effective Model for Gene...</td>\n",
       "      <td>https://proceedings.mlr.press/v161/daghaghi21a...</td>\n",
       "      <td>Zero-Shot Learning (ZSL) is a classification t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Co-Representation Network for Generalized Zero...</td>\n",
       "      <td>http://proceedings.mlr.press/v97/zhang19l/zhan...</td>\n",
       "      <td>Zero-shot learning (ZSL) is an important field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multi-Label Generalized Zero Shot Learning for...</td>\n",
       "      <td>https://proceedings.mlr.press/v149/hayat21a/ha...</td>\n",
       "      <td>(2021), our network can assign multiple labels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A simple and effective model for generalized z...</td>\n",
       "      <td>https://proceedings.mlr.press/v161/daghaghi21a...</td>\n",
       "      <td>Zero-Shot Learning (ZSL) is a classification t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Max-Margin Zero-Shot Learning for Multi-class ...</td>\n",
       "      <td>http://proceedings.mlr.press/v38/li15d.pdf</td>\n",
       "      <td>Zero-shot learning addresses the challenging p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zero-Shot Task Generalization with Multi-Task ...</td>\n",
       "      <td>http://proceedings.mlr.press/v70/oh17a/oh17a.pdf</td>\n",
       "      <td>Generalization: Pre-training of skills can onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Evolving Semantic Prototype Improves Generativ...</td>\n",
       "      <td>https://proceedings.mlr.press/v202/chen23l/che...</td>\n",
       "      <td>In zero-shot learning (ZSL), generative method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zero-Shot Knowledge Distillation in Deep Networks</td>\n",
       "      <td>http://proceedings.mlr.press/v97/nayak19a/naya...</td>\n",
       "      <td>Abstract. Knowledge distillation deals with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Context-Aware Zero-Shot Learning for Object Re...</td>\n",
       "      <td>http://proceedings.mlr.press/v97/zablocki19a/z...</td>\n",
       "      <td>Abstract. Zero-Shot Learning (ZSL) aims at cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Zero-Shot Knowledge Distillation in Deep Networks</td>\n",
       "      <td>http://proceedings.mlr.press/v97/nayak19a/naya...</td>\n",
       "      <td>Existing approaches use either the training da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Zero-Shot Text-to-Image Generation</td>\n",
       "      <td>https://proceedings.mlr.press/v139/ramesh21a.html</td>\n",
       "      <td>Proceedings of the 38th International Conferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Context-Aware Zero-Shot Learning for Object Re...</td>\n",
       "      <td>https://proceedings.mlr.press/v97/zablocki19a....</td>\n",
       "      <td>Following the intuitive principle that objects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Zero-Shot Task Generalization with Robotic Imi...</td>\n",
       "      <td>https://proceedings.mlr.press/v164/jang22a/jan...</td>\n",
       "      <td>Our main contribution is an empirical study of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Co-Representation Network for Generalized Zero...</td>\n",
       "      <td>https://proceedings.mlr.press/v97/zhang19l.html</td>\n",
       "      <td>The network consists of a cooperation module f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Zero-shot AutoML with Pretrained Models</td>\n",
       "      <td>https://proceedings.mlr.press/v162/ozturk22a.html</td>\n",
       "      <td>Proceedings of the 39th International Conferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Zero-Shot Knowledge Distillation from a Decisi...</td>\n",
       "      <td>https://proceedings.mlr.press/v139/wang21a.html</td>\n",
       "      <td>Proceedings of the 38th International Conferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Zero-Shot Reward Specification via Grounded Na...</td>\n",
       "      <td>https://proceedings.mlr.press/v162/mahmoudieh2...</td>\n",
       "      <td>Reward signals in reinforcement learning are e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>“Other-Play” for Zero-Shot Coordination</td>\n",
       "      <td>https://proceedings.mlr.press/v119/hu20a.html</td>\n",
       "      <td>Standard Multi-Agent Reinforcement Learning (M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>``Other-Play'' for Zero-Shot Coordination</td>\n",
       "      <td>http://proceedings.mlr.press/v119/hu20a/hu20a.pdf</td>\n",
       "      <td>Standard Multi-Agent Reinforcement. Learning (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   An embarrassingly simple approach to zero-shot...   \n",
       "1   SDM-Net: A Simple and Effective Model for Gene...   \n",
       "2   Co-Representation Network for Generalized Zero...   \n",
       "3   Multi-Label Generalized Zero Shot Learning for...   \n",
       "4   A simple and effective model for generalized z...   \n",
       "5   Max-Margin Zero-Shot Learning for Multi-class ...   \n",
       "6   Zero-Shot Task Generalization with Multi-Task ...   \n",
       "7   Evolving Semantic Prototype Improves Generativ...   \n",
       "8   Zero-Shot Knowledge Distillation in Deep Networks   \n",
       "9   Context-Aware Zero-Shot Learning for Object Re...   \n",
       "10  Zero-Shot Knowledge Distillation in Deep Networks   \n",
       "11                 Zero-Shot Text-to-Image Generation   \n",
       "12  Context-Aware Zero-Shot Learning for Object Re...   \n",
       "13  Zero-Shot Task Generalization with Robotic Imi...   \n",
       "14  Co-Representation Network for Generalized Zero...   \n",
       "15            Zero-shot AutoML with Pretrained Models   \n",
       "16  Zero-Shot Knowledge Distillation from a Decisi...   \n",
       "17  Zero-Shot Reward Specification via Grounded Na...   \n",
       "18            “Other-Play” for Zero-Shot Coordination   \n",
       "19          ``Other-Play'' for Zero-Shot Coordination   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://proceedings.mlr.press/v37/romera-pared...   \n",
       "1   https://proceedings.mlr.press/v161/daghaghi21a...   \n",
       "2   http://proceedings.mlr.press/v97/zhang19l/zhan...   \n",
       "3   https://proceedings.mlr.press/v149/hayat21a/ha...   \n",
       "4   https://proceedings.mlr.press/v161/daghaghi21a...   \n",
       "5          http://proceedings.mlr.press/v38/li15d.pdf   \n",
       "6    http://proceedings.mlr.press/v70/oh17a/oh17a.pdf   \n",
       "7   https://proceedings.mlr.press/v202/chen23l/che...   \n",
       "8   http://proceedings.mlr.press/v97/nayak19a/naya...   \n",
       "9   http://proceedings.mlr.press/v97/zablocki19a/z...   \n",
       "10  http://proceedings.mlr.press/v97/nayak19a/naya...   \n",
       "11  https://proceedings.mlr.press/v139/ramesh21a.html   \n",
       "12  https://proceedings.mlr.press/v97/zablocki19a....   \n",
       "13  https://proceedings.mlr.press/v164/jang22a/jan...   \n",
       "14    https://proceedings.mlr.press/v97/zhang19l.html   \n",
       "15  https://proceedings.mlr.press/v162/ozturk22a.html   \n",
       "16    https://proceedings.mlr.press/v139/wang21a.html   \n",
       "17  https://proceedings.mlr.press/v162/mahmoudieh2...   \n",
       "18      https://proceedings.mlr.press/v119/hu20a.html   \n",
       "19  http://proceedings.mlr.press/v119/hu20a/hu20a.pdf   \n",
       "\n",
       "                                              snippet  \n",
       "0   Zero-shot learning consists in learning how to...  \n",
       "1   Zero-Shot Learning (ZSL) is a classification t...  \n",
       "2   Zero-shot learning (ZSL) is an important field...  \n",
       "3   (2021), our network can assign multiple labels...  \n",
       "4   Zero-Shot Learning (ZSL) is a classification t...  \n",
       "5   Zero-shot learning addresses the challenging p...  \n",
       "6   Generalization: Pre-training of skills can onl...  \n",
       "7   In zero-shot learning (ZSL), generative method...  \n",
       "8   Abstract. Knowledge distillation deals with th...  \n",
       "9   Abstract. Zero-Shot Learning (ZSL) aims at cla...  \n",
       "10  Existing approaches use either the training da...  \n",
       "11  Proceedings of the 38th International Conferen...  \n",
       "12  Following the intuitive principle that objects...  \n",
       "13  Our main contribution is an empirical study of...  \n",
       "14  The network consists of a cooperation module f...  \n",
       "15  Proceedings of the 39th International Conferen...  \n",
       "16  Proceedings of the 38th International Conferen...  \n",
       "17  Reward signals in reinforcement learning are e...  \n",
       "18  Standard Multi-Agent Reinforcement Learning (M...  \n",
       "19  Standard Multi-Agent Reinforcement. Learning (...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmlr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3a810",
   "metadata": {},
   "source": [
    "### Get relevant papers from NeurlPS based on keyword ( based on serp_api )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33f8ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page : 1\n",
      "https://serpapi.com/search\n",
      "Page : 2\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "### Get relevant papers from nips based on keyword ( based on serp_api )\n",
    "nips_result = paper_engine.nips('Zero-shot learning', max_pages = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "debd24cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot Learning Through Cross-Modal Transfer</td>\n",
       "      <td>http://papers.nips.cc/paper/5027-zero-shot-lea...</td>\n",
       "      <td>This work introduces a model that can recogniz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero-shot Learning via Simultaneous Generating...</td>\n",
       "      <td>http://papers.nips.cc/paper/8300-zero-shot-lea...</td>\n",
       "      <td>Zero-shot learning (ZSL) is a learning paradig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zero-shot recognition with unreliable attributes</td>\n",
       "      <td>https://papers.nips.cc/paper/5290-zero-shot-re...</td>\n",
       "      <td>We propose a novel random forest approach to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSVA: Hierarchical Semantic-Visual Adaptation ...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2021/...</td>\n",
       "      <td>Zero-shot learning (ZSL) tackles the unseen cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zero-Shot Semantic Segmentation</td>\n",
       "      <td>https://papers.nips.cc/paper/8338-zero-shot-se...</td>\n",
       "      <td>In this paper, we introduce the new task of ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Generating Training Data with Language Models</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2022/...</td>\n",
       "      <td>In this paper, we present a simple approach th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2022/...</td>\n",
       "      <td>Pretrained large language models (LLMs) are wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Generalized Zero-Shot Learning with Deep Calib...</td>\n",
       "      <td>https://papers.nips.cc/paper/7471-generalized-...</td>\n",
       "      <td>In this paper, we study generalized zero-shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A causal view of compositional zero-shot recog...</td>\n",
       "      <td>https://papers.nips.cc/paper/2020/file/1010ced...</td>\n",
       "      <td>Compositional zero-shot recognition is the pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zero-shot Learning with Semantic Output Codes ...</td>\n",
       "      <td>https://papers.nips.cc/paper/3650-zero-shot-le...</td>\n",
       "      <td>As a case study, we build a SOC classifier for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Zero-shot Learning with Semantic Output Codes ...</td>\n",
       "      <td>https://papers.nips.cc/paper/3650-zero-shot-le...</td>\n",
       "      <td>Abstract. We consider the problem of zero-shot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Emergence of Objectness: Learning Zero-sho...</td>\n",
       "      <td>https://papers.nips.cc/paper/2021/hash/6d9cb7d...</td>\n",
       "      <td>Humans can easily detect and segment moving ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Large Language Models are Zero-Shot Reasoners</td>\n",
       "      <td>http://papers.nips.cc/paper_files/paper/2022/h...</td>\n",
       "      <td>While these successes are often attributed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Reviews: Zero-shot Learning via Simultaneous G...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2019/...</td>\n",
       "      <td>The addresses zero-shot learning by an EM proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Zero-shot Learning via Simultaneous Generating...</td>\n",
       "      <td>https://papers.nips.cc/paper/8300-zero-shot-le...</td>\n",
       "      <td>To overcome the absence of training data for u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Compositional Zero-Shot Learning via Fine-Grai...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2020/...</td>\n",
       "      <td>We develop a novel generative model for zero-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Domain-Invariant Projection Learning for Zero-...</td>\n",
       "      <td>https://papers.nips.cc/paper/7380-domain-invar...</td>\n",
       "      <td>Zero-shot learning (ZSL) aims to recognize uns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fine-Grained Zero-Shot Learning with DNA as Si...</td>\n",
       "      <td>https://papers.nips.cc/paper/2021/hash/a18630a...</td>\n",
       "      <td>On a newly compiled fine-grained insect datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Transductive Zero-Shot Learning with Visual St...</td>\n",
       "      <td>https://papers.nips.cc/paper/9188-transductive...</td>\n",
       "      <td>To recognize objects of the unseen classes, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fine-Grained Zero-Shot Learning with DNA as Si...</td>\n",
       "      <td>https://papers.nips.cc/paper_files/paper/2021/...</td>\n",
       "      <td>Fine-grained zero-shot learning task requires ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0     Zero-Shot Learning Through Cross-Modal Transfer   \n",
       "1   Zero-shot Learning via Simultaneous Generating...   \n",
       "2    Zero-shot recognition with unreliable attributes   \n",
       "3   HSVA: Hierarchical Semantic-Visual Adaptation ...   \n",
       "4                     Zero-Shot Semantic Segmentation   \n",
       "5       Generating Training Data with Language Models   \n",
       "6       Large Language Models are Zero-Shot Reasoners   \n",
       "7   Generalized Zero-Shot Learning with Deep Calib...   \n",
       "8   A causal view of compositional zero-shot recog...   \n",
       "9   Zero-shot Learning with Semantic Output Codes ...   \n",
       "10  Zero-shot Learning with Semantic Output Codes ...   \n",
       "11  The Emergence of Objectness: Learning Zero-sho...   \n",
       "12      Large Language Models are Zero-Shot Reasoners   \n",
       "13  Reviews: Zero-shot Learning via Simultaneous G...   \n",
       "14  Zero-shot Learning via Simultaneous Generating...   \n",
       "15  Compositional Zero-Shot Learning via Fine-Grai...   \n",
       "16  Domain-Invariant Projection Learning for Zero-...   \n",
       "17  Fine-Grained Zero-Shot Learning with DNA as Si...   \n",
       "18  Transductive Zero-Shot Learning with Visual St...   \n",
       "19  Fine-Grained Zero-Shot Learning with DNA as Si...   \n",
       "\n",
       "                                                 link  \\\n",
       "0   http://papers.nips.cc/paper/5027-zero-shot-lea...   \n",
       "1   http://papers.nips.cc/paper/8300-zero-shot-lea...   \n",
       "2   https://papers.nips.cc/paper/5290-zero-shot-re...   \n",
       "3   https://papers.nips.cc/paper_files/paper/2021/...   \n",
       "4   https://papers.nips.cc/paper/8338-zero-shot-se...   \n",
       "5   https://papers.nips.cc/paper_files/paper/2022/...   \n",
       "6   https://papers.nips.cc/paper_files/paper/2022/...   \n",
       "7   https://papers.nips.cc/paper/7471-generalized-...   \n",
       "8   https://papers.nips.cc/paper/2020/file/1010ced...   \n",
       "9   https://papers.nips.cc/paper/3650-zero-shot-le...   \n",
       "10  https://papers.nips.cc/paper/3650-zero-shot-le...   \n",
       "11  https://papers.nips.cc/paper/2021/hash/6d9cb7d...   \n",
       "12  http://papers.nips.cc/paper_files/paper/2022/h...   \n",
       "13  https://papers.nips.cc/paper_files/paper/2019/...   \n",
       "14  https://papers.nips.cc/paper/8300-zero-shot-le...   \n",
       "15  https://papers.nips.cc/paper_files/paper/2020/...   \n",
       "16  https://papers.nips.cc/paper/7380-domain-invar...   \n",
       "17  https://papers.nips.cc/paper/2021/hash/a18630a...   \n",
       "18  https://papers.nips.cc/paper/9188-transductive...   \n",
       "19  https://papers.nips.cc/paper_files/paper/2021/...   \n",
       "\n",
       "                                              snippet  \n",
       "0   This work introduces a model that can recogniz...  \n",
       "1   Zero-shot learning (ZSL) is a learning paradig...  \n",
       "2   We propose a novel random forest approach to t...  \n",
       "3   Zero-shot learning (ZSL) tackles the unseen cl...  \n",
       "4   In this paper, we introduce the new task of ze...  \n",
       "5   In this paper, we present a simple approach th...  \n",
       "6   Pretrained large language models (LLMs) are wi...  \n",
       "7   In this paper, we study generalized zero-shot ...  \n",
       "8   Compositional zero-shot recognition is the pro...  \n",
       "9   As a case study, we build a SOC classifier for...  \n",
       "10  Abstract. We consider the problem of zero-shot...  \n",
       "11  Humans can easily detect and segment moving ob...  \n",
       "12  While these successes are often attributed to ...  \n",
       "13  The addresses zero-shot learning by an EM proc...  \n",
       "14  To overcome the absence of training data for u...  \n",
       "15  We develop a novel generative model for zero-s...  \n",
       "16  Zero-shot learning (ZSL) aims to recognize uns...  \n",
       "17  On a newly compiled fine-grained insect datase...  \n",
       "18  To recognize objects of the unseen classes, mo...  \n",
       "19  Fine-grained zero-shot learning task requires ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nips_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c173a",
   "metadata": {},
   "source": [
    "#### Get relevant papers from any conference website based on keyword ( based on serp_api )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "739e2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page : 1\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "websit_result = paper_engine.custom_search(url       = 'https://link.springer.com', \n",
    "                                           keyword   = 'Zero-shot learning', \n",
    "                                           max_pages = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56890f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot Learning via Visual Abstraction</td>\n",
       "      <td>https://link.springer.com/chapter/10.1007/978-...</td>\n",
       "      <td>Abstract. One of the main challenges in learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Class-Incremental Generalized Zero-Shot Learning</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s110...</td>\n",
       "      <td>Zero-Shot Learning (ZSL) focuses on transferri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning semantic ambiguities for zero-shot le...</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s110...</td>\n",
       "      <td>Abstract. Zero-shot learning (ZSL) aims at rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A study on zero-shot learning from semantic vi...</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s003...</td>\n",
       "      <td>The proposed method uses a joint learning fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Information bottleneck and selective noise sup...</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s109...</td>\n",
       "      <td>Zero-shot learning (ZSL) aims to recognize nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From zero-shot machine learning to zero-day at...</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s102...</td>\n",
       "      <td>Zero-shot learning (ZSL) is an emerging method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zero-shot learning via self-organizing maps</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s005...</td>\n",
       "      <td>Instead of starting with completely random vec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dynamic visual-guided selection for zero-shot ...</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s112...</td>\n",
       "      <td>Zero-shot learning (ZSL) methods currently emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>An effective zero-shot learning approach for i...</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s104...</td>\n",
       "      <td>ZSL provides a powerful tool for solving the u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Semantics-Guided Intra-Category Knowledge Tran...</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s112...</td>\n",
       "      <td>Zero-shot learning (ZSL) requires one to assoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0          Zero-Shot Learning via Visual Abstraction   \n",
       "1   Class-Incremental Generalized Zero-Shot Learning   \n",
       "2  Learning semantic ambiguities for zero-shot le...   \n",
       "3  A study on zero-shot learning from semantic vi...   \n",
       "4  Information bottleneck and selective noise sup...   \n",
       "5  From zero-shot machine learning to zero-day at...   \n",
       "6        Zero-shot learning via self-organizing maps   \n",
       "7  Dynamic visual-guided selection for zero-shot ...   \n",
       "8  An effective zero-shot learning approach for i...   \n",
       "9  Semantics-Guided Intra-Category Knowledge Tran...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://link.springer.com/chapter/10.1007/978-...   \n",
       "1  https://link.springer.com/article/10.1007/s110...   \n",
       "2  https://link.springer.com/article/10.1007/s110...   \n",
       "3  https://link.springer.com/article/10.1007/s003...   \n",
       "4  https://link.springer.com/article/10.1007/s109...   \n",
       "5  https://link.springer.com/article/10.1007/s102...   \n",
       "6  https://link.springer.com/article/10.1007/s005...   \n",
       "7  https://link.springer.com/article/10.1007/s112...   \n",
       "8  https://link.springer.com/article/10.1007/s104...   \n",
       "9  https://link.springer.com/article/10.1007/s112...   \n",
       "\n",
       "                                             snippet  \n",
       "0  Abstract. One of the main challenges in learni...  \n",
       "1  Zero-Shot Learning (ZSL) focuses on transferri...  \n",
       "2  Abstract. Zero-shot learning (ZSL) aims at rec...  \n",
       "3  The proposed method uses a joint learning fram...  \n",
       "4  Zero-shot learning (ZSL) aims to recognize nov...  \n",
       "5  Zero-shot learning (ZSL) is an emerging method...  \n",
       "6  Instead of starting with completely random vec...  \n",
       "7  Zero-shot learning (ZSL) methods currently emp...  \n",
       "8  ZSL provides a powerful tool for solving the u...  \n",
       "9  Zero-shot learning (ZSL) requires one to assoc...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5d81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b337d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18647d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619598d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
